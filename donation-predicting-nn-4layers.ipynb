{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "import ast\n",
    "\n",
    "# gets rid of scientific notation of arrays when printing\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# shows all columns \n",
    "pd.set_option('display.max_columns', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = .00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define activation functions\n",
    "# softmax activation function\n",
    "def softmax(x):\n",
    "    expx = np.exp(x - np.max(x))\n",
    "    return expx/np.sum(expx, axis=1, keepdims=True)\n",
    "\n",
    "# relu activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# derivative of relu activation function\n",
    "def drelu(x):\n",
    "    return np.where(x > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_accuracy(targets, predictions) :\n",
    "    total = len(predictions)\n",
    "    correct = 0\n",
    "    for i in range(total):\n",
    "        predicted_class = np.argmax(predictions[i])\n",
    "        if targets[i][predicted_class] == 1:\n",
    "           correct += 1\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "donordata = pd.read_csv('./data/clean-data.csv', index_col=None)\n",
    "\n",
    "traindata = donordata.sample(frac=.8, random_state=1)\n",
    "testdata = donordata.drop(traindata.index)\n",
    "\n",
    "train_target_output = traindata.target_output.values\n",
    "test_target_output = testdata.target_output.values\n",
    "\n",
    "traindata = traindata.drop(['target_output'], axis=1)\n",
    "traindata = traindata.set_index(['Contact_ID'], drop=True)\n",
    "testdata = testdata.drop(['target_output'], axis=1)\n",
    "testdata = testdata.set_index(['Contact_ID'], drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the target output for the train data\n",
    "first = True\n",
    "for x in train_target_output:\n",
    "    if first:\n",
    "        train_one_hot_encoded = np.array([ast.literal_eval(x)])\n",
    "        first = False\n",
    "    else:\n",
    "        train_one_hot_encoded = np.append(train_one_hot_encoded, [ast.literal_eval(x)],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the target output for the test data\n",
    "first = True\n",
    "for x in test_target_output:\n",
    "    if first:\n",
    "        test_one_hot_encoded = np.array([ast.literal_eval(x)])\n",
    "        first = False\n",
    "    else:\n",
    "        test_one_hot_encoded = np.append(test_one_hot_encoded, [ast.literal_eval(x)],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = len(traindata.columns)\n",
    "\n",
    "# initialize weights using He initialization\n",
    "np.random.seed(1)\n",
    "weights_1 = np.random.randn(num_inputs,40)*np.sqrt(2/num_inputs)\n",
    "weights_2 = np.random.randn(40,40)*np.sqrt(2/40)\n",
    "weights_3 = np.random.randn(40,40)*np.sqrt(2/40)\n",
    "weights_4 = np.random.randn(40,14)*np.sqrt(2/40)\n",
    "\n",
    "# initialize bias\n",
    "bias_1 = 0\n",
    "bias_2 = 0\n",
    "bias_3 = 0\n",
    "bias_4 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## takes in the inputs and adjusts the weights accordingly\n",
    "# If train is false, it makes a prediction and doesn't adjust the weights\n",
    "def train(batch, target_outputs, train):\n",
    "    global weights_4\n",
    "    global weights_3\n",
    "    global weights_2\n",
    "    global weights_1\n",
    "    global bias_1\n",
    "    global bias_2\n",
    "    global bias_3\n",
    "    global bias_4\n",
    "    \n",
    "    ## feedforward\n",
    "    layer_1_output = relu(np.dot(batch, weights_1) + bias_1)\n",
    "    layer_2_output = relu(np.dot(layer_1_output, weights_2) + bias_2)\n",
    "    layer_3_output = relu(np.dot(layer_2_output, weights_3) + bias_3)\n",
    "    output = softmax(np.dot(layer_3_output, weights_4) + bias_4)\n",
    "\n",
    "    if train:\n",
    "        ## backprop\n",
    "        # output layer\n",
    "        dcost_dpred = target_outputs - output\n",
    "        dz_dw4 = layer_3_output\n",
    "        \n",
    "        dcost_dw4 = np.dot(dz_dw4.T, dcost_dpred)\n",
    "        dcost_b4 = dcost_dpred\n",
    "        \n",
    "        # layer 3\n",
    "        dcost_doutput3 = np.dot(dcost_dpred, weights_4.T)\n",
    "        doutput3_dz = drelu(layer_3_output)\n",
    "        dz_dw3 = layer_2_output\n",
    "        \n",
    "        dcost_dw3 = np.dot(dz_dw3.T, dcost_doutput3 * doutput3_dz)\n",
    "        dcost_b3 = dcost_doutput3 * doutput3_dz\n",
    "        \n",
    "        # layer 2\n",
    "        dcost_doutput2 = np.dot(dcost_doutput3 * doutput3_dz, weights_3.T)\n",
    "        doutput2_dz = drelu(layer_2_output)\n",
    "        dz_dw2 = layer_1_output\n",
    "        \n",
    "        dcost_dw2 = np.dot(dz_dw2.T, dcost_doutput2 * doutput2_dz)\n",
    "        dcost_b2 = dcost_doutput2 * doutput2_dz\n",
    "\n",
    "        # layer 1\n",
    "        dcost_doutput1 = np.dot(dcost_doutput2 * doutput2_dz, weights_2.T)\n",
    "        doutput1_dz = drelu(layer_1_output)\n",
    "        dz_dw1 = batch\n",
    "        \n",
    "        dcost_dw1 = np.dot(dz_dw1.T, dcost_doutput1 * doutput1_dz)\n",
    "        dcost_b1 = dcost_doutput1 * doutput1_dz\n",
    "        \n",
    "        # update weights\n",
    "        weights_4 += dcost_dw4 * learning_rate\n",
    "        weights_3 += dcost_dw3 * learning_rate\n",
    "        weights_2 += dcost_dw2 * learning_rate\n",
    "        weights_1 += dcost_dw1 * learning_rate\n",
    "\n",
    "        # update biases\n",
    "        bias_4 += np.average(dcost_b3) * learning_rate\n",
    "        bias_3 += np.average(dcost_b3) * learning_rate\n",
    "        bias_2 += np.average(dcost_b2) * learning_rate\n",
    "        bias_1 += np.average(dcost_b1) * learning_rate\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06870638754696726\n",
      "0.6634460547504025\n",
      "0.6714975845410628\n",
      "0.6790123456790124\n",
      "0.6822329575952765\n",
      "0.6843800322061192\n",
      "0.6854535695115406\n",
      "0.6849168008588299\n",
      "0.6849168008588299\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7168658505d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_one_hot_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_one_hot_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0merror_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-ba97840441f4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(batch, target_outputs, train)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m## feedforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mlayer_1_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbias_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mlayer_2_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_1_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbias_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mlayer_3_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_2_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbias_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "error_list = []\n",
    "index = []\n",
    "train_data = preprocessing.scale(traindata.values)\n",
    "test_data = preprocessing.scale(testdata.values)\n",
    "for i in range(1001):\n",
    "    train_output = train(train_data, train_one_hot_encoded, True)\n",
    "    error = log_loss(train_one_hot_encoded, train_output)\n",
    "    error_list.append(error)\n",
    "    index.append(i)\n",
    "    if (i % 100) == 0:\n",
    "        test_output = train(test_data, test_one_hot_encoded, False)\n",
    "        accuracy = classification_accuracy(test_one_hot_encoded, test_output)\n",
    "        print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot error vs. epoch\n",
    "plt.plot(index, error_list)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cross Entropy Loss')\n",
    "plt.title('Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification accuracy of training data\n",
    "classification_accuracy(train_one_hot_encoded, train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification accuracy of test data\n",
    "test_data = preprocessing.scale(testdata.values)\n",
    "test_output = train(test_data, test_one_hot_encoded, False)\n",
    "classification_accuracy(test_one_hot_encoded, test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#30: 0.9193430061949788\n",
    "#40: 0.9215030974894033\n",
    "    # with 3 layers: 0.9226035213563744\n",
    "#50: 0.9227257906749267\n",
    "    # with 3 layers: 0.926149331594392\n",
    "    # with 4 layers: 0.9261900880339093\n",
    "#61: 0.9225627649168568\n",
    "    # with 3 layers: 0.9268421910661885"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
